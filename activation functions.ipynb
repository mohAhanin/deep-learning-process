{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7db042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f44789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99cdf2f",
   "metadata": {},
   "source": [
    "# 1. Sigmoid:\n",
    "Pros:\n",
    "The sigmoid activation function maps inputs to the range of 0 and 1, which makes it useful for binary classification problems.\n",
    "The sigmoid function is differentiable, making it suitable for use in backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05611464",
   "metadata": {},
   "source": [
    "Cons:\n",
    "The sigmoid function saturates for large inputs, meaning that the gradients can become very small, leading to the vanishing gradient problem.\n",
    "The sigmoid function outputs are not zero-centered, which can lead to slow convergence when using optimization algorithms such as gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a05853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90bef3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11920292202211755"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a1573",
   "metadata": {},
   "source": [
    "# 2. ReLU:\n",
    "Pros:\n",
    "ReLU is computationally efficient as it simply involves thresholding the input.\n",
    "ReLU can avoid the vanishing gradient problem that is encountered with the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379419e",
   "metadata": {},
   "source": [
    "Cons:\n",
    "ReLU can produce dead neurons, meaning that some neurons may produce zero outputs for all inputs, leading to a loss of information.\n",
    "ReLU is not differentiable at 0, which can pose challenges for optimization algorithms such as gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b695f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa0e5fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3455"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(3455)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3b020",
   "metadata": {},
   "source": [
    "# 3. Tanh:\n",
    "Pros:\n",
    "The tanh function produces outputs with zero mean and unit variance, which can improve the performance of the network.\n",
    "The tanh function is differentiable, making it suitable for use in backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de9ebf",
   "metadata": {},
   "source": [
    "Cons:\n",
    "The tanh function saturates for large inputs, meaning that the gradients can become very small, leading to the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0908d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c440ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615941559557649"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfcc3fe",
   "metadata": {},
   "source": [
    "# 4. Softmax:\n",
    "Pros:\n",
    "Softmax activation is ideal for multiclass classification problems as it maps its inputs to a probability distribution over multiple classes.\n",
    "Softmax is differentiable, making it suitable for use in backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad154b",
   "metadata": {},
   "source": [
    "Cons:\n",
    "Softmax is computationally expensive as it requires normalizing the exponential of the inputs, which can be computationally intensive for large inputs.\n",
    "Softmax is sensitive to the scale of the inputs, meaning that large inputs can dominate the outputs, leading to a loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3006ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51c5c9",
   "metadata": {},
   "source": [
    "# 5. Leaky ReLU:\n",
    "Pros:\n",
    "Leaky ReLU can avoid the dying neurons problem encountered with the ReLU activation function.\n",
    "Leaky ReLU is differentiable, making it suitable for use in backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1bc1a",
   "metadata": {},
   "source": [
    "Cons:\n",
    "Leaky ReLU requires the choice of a hyperparameter, which determines the slope of the function for negative inputs.\n",
    "Leaky ReLU is not as computationally efficient as the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e6334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha * x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626fea84",
   "metadata": {},
   "source": [
    "# 6. Swish:\n",
    "Pros:\n",
    "Swish has been shown to outperform ReLU on some tasks.\n",
    "Swish is differentiable, making it suitable for use in backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f183f8",
   "metadata": {},
   "source": [
    "Cons:\n",
    "Swish requires the evaluation of both the sigmoid function and the input, making it computationally more expensive than ReLU.\n",
    "Swish requires the choice of a hyperparameter, which can be challenging to set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e288874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd72fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
